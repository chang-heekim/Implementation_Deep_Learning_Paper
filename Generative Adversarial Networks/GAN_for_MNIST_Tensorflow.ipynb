{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_for_MNIST-Tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMJ3zlcl/v9oILn7n9BDf0k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chang-heekim/Implementation_Deep_Learning_Paper/blob/main/Generative%20Adversarial%20Networks/GAN_for_MNIST_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Necessary Library"
      ],
      "metadata": {
        "id": "4WdsmbdDZRh3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXpd5RnUGPQE"
      },
      "outputs": [],
      "source": [
        "import os, time  \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, BatchNormalization, LeakyReLU\n",
        "from tensorflow.keras import Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up hyper parameters & Load MNist dataset"
      ],
      "metadata": {
        "id": "p1cW53ehZVcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 100\n",
        "batch_size = 256\n",
        "epochs = 500"
      ],
      "metadata": {
        "id": "MgR6FDxXHIKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "train_x = train_x.reshape(train_x.shape[0], 28, 28, 1).astype('float32')\n",
        "test_x = test_x.reshape(test_x.shape[0], 28, 28, 1).astype('float32')\n",
        "\n",
        "train_x = (train_x - 127.5) / 127.5\n",
        "test_x = (test_x  - 127.5) / 127.5\n",
        "print(\"train_x.shape = {}\".format(train_x.shape))\n",
        "print(\"test_x.shape = {}\".format(test_x.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bU8kdh97GcQG",
        "outputId": "8756b375-d4e0-4606-dd58-8686f1a21c15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_x.shape = (60000, 28, 28, 1)\n",
            "test_x.shape = (10000, 28, 28, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(1024).batch(batch_size, drop_remainder=True)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).batch(batch_size, drop_remainder=True)"
      ],
      "metadata": {
        "id": "x6o2hzLeNear"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Generator & Discriminator"
      ],
      "metadata": {
        "id": "MQ1arUjIZdOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generator():\n",
        "    inputs = Input((latent_dim,))\n",
        "\n",
        "    x = Dense(128, use_bias=False)(inputs)\n",
        "    x = BatchNormalization(momentum=0.8)(x)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = Dense(256, use_bias=False)(x)\n",
        "    x = BatchNormalization(momentum=0.8)(x)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = Dense(512, use_bias=False)(x)\n",
        "    x = BatchNormalization(momentum=0.8)(x)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = Dense(1024, use_bias=False)(x)\n",
        "    x = BatchNormalization(momentum=0.8)(x)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = Dense(28*28, activation='tanh')(x)\n",
        "    out = Reshape((28, 28, 1))(x)\n",
        "\n",
        "    model = Model(inputs, out, name='Generator')\n",
        "    return model\n",
        "\n",
        "def discriminator():\n",
        "    inputs = Input((28, 28, 1))\n",
        "\n",
        "    x = Flatten()(inputs)\n",
        "    x = Dense(512)(x)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "    x = Dense(256)(x)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "    out = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs, out, name='Discriminator')\n",
        "    return model"
      ],
      "metadata": {
        "id": "oYSPIeJ5Gr1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = generator()\n",
        "discriminator = discriminator()\n",
        "\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "g_optimizer = tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n",
        "d_optimizer = tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryvV62cOIdPc",
        "outputId": "80a205d1-b934-46c2-b6a1-b0e95213c56e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Train Function"
      ],
      "metadata": {
        "id": "ylCWRfrSZlyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(epoch, images, loss_fn, g_optimizer, d_optimizer):\n",
        "    with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n",
        "        z = tf.random.normal([batch_size, latent_dim], 0, 1)\n",
        "        g_images = generator(z, training=True)\n",
        "\n",
        "        real_out = discriminator(images)\n",
        "        fake_out = discriminator(g_images)\n",
        "\n",
        "        g_loss = loss_fn(tf.ones_like(fake_out), fake_out)\n",
        "\n",
        "        real_loss = loss_fn(tf.ones_like(real_out), real_out)\n",
        "        fake_loss = loss_fn(tf.zeros_like(fake_out), fake_out)\n",
        "\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "    g_gradients = g_tape.gradient(g_loss, generator.trainable_variables)\n",
        "    d_gradients = d_tape.gradient(d_loss, discriminator.trainable_variables)\n",
        "\n",
        "    g_optimizer.apply_gradients(zip(g_gradients, generator.trainable_variables))\n",
        "    d_optimizer.apply_gradients(zip(d_gradients, discriminator.trainable_variables))\n",
        "\n",
        "    return g_loss, d_loss"
      ],
      "metadata": {
        "id": "xJVCX6oSJWBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "LqBvSqkyZoJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, 1 + epochs):\n",
        "    for images, _ in train_dataset:\n",
        "        g_loss, d_loss = train_step(epoch, images, loss_fn, g_optimizer, d_optimizer)\n",
        "    print(f'[Epoch {epoch} / {epochs}], G Loss: {g_loss}, D Loss: {d_loss}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jV5iuQuTeqh",
        "outputId": "92bc43d4-331a-4828-8cb1-50e74b7f798a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1 / 500], G Loss: 0.8253350257873535, D Loss: 0.6052611470222473\n",
            "[Epoch 2 / 500], G Loss: 0.9208213090896606, D Loss: 0.5618720054626465\n",
            "[Epoch 3 / 500], G Loss: 0.5603691935539246, D Loss: 0.627212643623352\n",
            "[Epoch 4 / 500], G Loss: 0.6465646028518677, D Loss: 0.6001654863357544\n",
            "[Epoch 5 / 500], G Loss: 1.1401996612548828, D Loss: 0.5563600063323975\n",
            "[Epoch 6 / 500], G Loss: 0.9425413608551025, D Loss: 0.5642002820968628\n",
            "[Epoch 7 / 500], G Loss: 0.8560358285903931, D Loss: 0.5433155298233032\n",
            "[Epoch 8 / 500], G Loss: 0.6848170757293701, D Loss: 0.5863988399505615\n",
            "[Epoch 9 / 500], G Loss: 0.2654040455818176, D Loss: 0.8312289118766785\n",
            "[Epoch 10 / 500], G Loss: 0.8192702531814575, D Loss: 0.5307362079620361\n",
            "[Epoch 11 / 500], G Loss: 1.020681381225586, D Loss: 0.5286600589752197\n",
            "[Epoch 12 / 500], G Loss: 0.9391814470291138, D Loss: 0.5690574049949646\n",
            "[Epoch 13 / 500], G Loss: 0.9812250137329102, D Loss: 0.5457456111907959\n",
            "[Epoch 14 / 500], G Loss: 0.7231684327125549, D Loss: 0.5924824476242065\n",
            "[Epoch 15 / 500], G Loss: 1.0907533168792725, D Loss: 0.5941406488418579\n",
            "[Epoch 16 / 500], G Loss: 1.0525916814804077, D Loss: 0.5353386402130127\n",
            "[Epoch 17 / 500], G Loss: 1.3781459331512451, D Loss: 0.5727013349533081\n",
            "[Epoch 18 / 500], G Loss: 0.5292105674743652, D Loss: 0.6347244381904602\n",
            "[Epoch 19 / 500], G Loss: 0.590735912322998, D Loss: 0.6497050523757935\n",
            "[Epoch 20 / 500], G Loss: 0.7618707418441772, D Loss: 0.5403591394424438\n",
            "[Epoch 21 / 500], G Loss: 1.3257830142974854, D Loss: 0.6275526881217957\n",
            "[Epoch 22 / 500], G Loss: 1.0710639953613281, D Loss: 0.5940779447555542\n",
            "[Epoch 23 / 500], G Loss: 1.0322296619415283, D Loss: 0.5968101024627686\n",
            "[Epoch 24 / 500], G Loss: 0.8626269698143005, D Loss: 0.5797734260559082\n",
            "[Epoch 25 / 500], G Loss: 1.0732395648956299, D Loss: 0.5925345420837402\n",
            "[Epoch 26 / 500], G Loss: 0.8732253909111023, D Loss: 0.5809475183486938\n",
            "[Epoch 27 / 500], G Loss: 0.9187240600585938, D Loss: 0.6045751571655273\n",
            "[Epoch 28 / 500], G Loss: 0.8619310259819031, D Loss: 0.580956220626831\n",
            "[Epoch 29 / 500], G Loss: 0.6388009786605835, D Loss: 0.6241596937179565\n",
            "[Epoch 30 / 500], G Loss: 1.2433239221572876, D Loss: 0.6014159917831421\n",
            "[Epoch 31 / 500], G Loss: 0.8317767381668091, D Loss: 0.5806452631950378\n",
            "[Epoch 32 / 500], G Loss: 0.8815689086914062, D Loss: 0.6120216846466064\n",
            "[Epoch 33 / 500], G Loss: 0.9754594564437866, D Loss: 0.573253333568573\n",
            "[Epoch 34 / 500], G Loss: 1.13365638256073, D Loss: 0.5804216861724854\n",
            "[Epoch 35 / 500], G Loss: 0.6526766419410706, D Loss: 0.6034172177314758\n",
            "[Epoch 36 / 500], G Loss: 1.0196030139923096, D Loss: 0.5960311889648438\n",
            "[Epoch 37 / 500], G Loss: 0.968059241771698, D Loss: 0.5733968019485474\n",
            "[Epoch 38 / 500], G Loss: 1.2180407047271729, D Loss: 0.5793444514274597\n",
            "[Epoch 39 / 500], G Loss: 1.0026164054870605, D Loss: 0.6170624494552612\n",
            "[Epoch 40 / 500], G Loss: 0.8399646878242493, D Loss: 0.621372401714325\n",
            "[Epoch 41 / 500], G Loss: 0.9707921743392944, D Loss: 0.556209921836853\n",
            "[Epoch 42 / 500], G Loss: 0.8413628935813904, D Loss: 0.5919042825698853\n",
            "[Epoch 43 / 500], G Loss: 0.7693405151367188, D Loss: 0.5966246128082275\n",
            "[Epoch 44 / 500], G Loss: 0.8005163073539734, D Loss: 0.6002223491668701\n",
            "[Epoch 45 / 500], G Loss: 0.749829888343811, D Loss: 0.6002294421195984\n",
            "[Epoch 46 / 500], G Loss: 1.0058828592300415, D Loss: 0.6061813235282898\n",
            "[Epoch 47 / 500], G Loss: 0.494411826133728, D Loss: 0.6922407150268555\n",
            "[Epoch 48 / 500], G Loss: 0.7242762446403503, D Loss: 0.5985245108604431\n",
            "[Epoch 49 / 500], G Loss: 1.3955636024475098, D Loss: 0.6769384145736694\n",
            "[Epoch 50 / 500], G Loss: 0.8781097531318665, D Loss: 0.6064527034759521\n",
            "[Epoch 51 / 500], G Loss: 0.6503702998161316, D Loss: 0.6287280321121216\n",
            "[Epoch 52 / 500], G Loss: 0.964083194732666, D Loss: 0.5858104228973389\n",
            "[Epoch 53 / 500], G Loss: 0.9587905406951904, D Loss: 0.5998989343643188\n",
            "[Epoch 54 / 500], G Loss: 1.0827215909957886, D Loss: 0.5714202523231506\n",
            "[Epoch 55 / 500], G Loss: 0.8395406603813171, D Loss: 0.5720248222351074\n",
            "[Epoch 56 / 500], G Loss: 0.8538369536399841, D Loss: 0.5981017351150513\n",
            "[Epoch 57 / 500], G Loss: 0.9949182868003845, D Loss: 0.6242541074752808\n",
            "[Epoch 58 / 500], G Loss: 0.660628080368042, D Loss: 0.6195625066757202\n",
            "[Epoch 59 / 500], G Loss: 0.8807939887046814, D Loss: 0.6061905026435852\n",
            "[Epoch 60 / 500], G Loss: 0.8133870959281921, D Loss: 0.5968231558799744\n",
            "[Epoch 61 / 500], G Loss: 0.7681984305381775, D Loss: 0.605392575263977\n",
            "[Epoch 62 / 500], G Loss: 0.5990802049636841, D Loss: 0.6471372842788696\n",
            "[Epoch 63 / 500], G Loss: 0.5695074796676636, D Loss: 0.6705816984176636\n",
            "[Epoch 64 / 500], G Loss: 0.8634278178215027, D Loss: 0.5899776220321655\n",
            "[Epoch 65 / 500], G Loss: 0.8629451990127563, D Loss: 0.586218535900116\n",
            "[Epoch 66 / 500], G Loss: 1.0673630237579346, D Loss: 0.5947973132133484\n",
            "[Epoch 67 / 500], G Loss: 0.9221930503845215, D Loss: 0.620800793170929\n",
            "[Epoch 68 / 500], G Loss: 0.8318420052528381, D Loss: 0.616475522518158\n",
            "[Epoch 69 / 500], G Loss: 1.1493215560913086, D Loss: 0.6397473216056824\n",
            "[Epoch 70 / 500], G Loss: 1.4490966796875, D Loss: 0.6865344643592834\n",
            "[Epoch 71 / 500], G Loss: 0.9124546051025391, D Loss: 0.5778936743736267\n",
            "[Epoch 72 / 500], G Loss: 0.8042593598365784, D Loss: 0.5985287427902222\n",
            "[Epoch 73 / 500], G Loss: 0.8122924566268921, D Loss: 0.5967028141021729\n",
            "[Epoch 74 / 500], G Loss: 0.9504472017288208, D Loss: 0.6055624485015869\n",
            "[Epoch 75 / 500], G Loss: 0.6280308961868286, D Loss: 0.6255810260772705\n",
            "[Epoch 76 / 500], G Loss: 0.7722164392471313, D Loss: 0.622576117515564\n",
            "[Epoch 77 / 500], G Loss: 1.007924199104309, D Loss: 0.5975650548934937\n",
            "[Epoch 78 / 500], G Loss: 1.0316221714019775, D Loss: 0.5775431394577026\n",
            "[Epoch 79 / 500], G Loss: 0.872912585735321, D Loss: 0.5720558166503906\n",
            "[Epoch 80 / 500], G Loss: 0.6052906513214111, D Loss: 0.6470106244087219\n",
            "[Epoch 81 / 500], G Loss: 0.889527440071106, D Loss: 0.6007333397865295\n",
            "[Epoch 82 / 500], G Loss: 0.638363242149353, D Loss: 0.6423994302749634\n",
            "[Epoch 83 / 500], G Loss: 1.0392765998840332, D Loss: 0.5865389704704285\n",
            "[Epoch 84 / 500], G Loss: 0.9228578209877014, D Loss: 0.5777599215507507\n",
            "[Epoch 85 / 500], G Loss: 0.7867752313613892, D Loss: 0.6264933347702026\n",
            "[Epoch 86 / 500], G Loss: 0.7503136396408081, D Loss: 0.6032105684280396\n",
            "[Epoch 87 / 500], G Loss: 1.0823785066604614, D Loss: 0.6157447099685669\n",
            "[Epoch 88 / 500], G Loss: 0.954125702381134, D Loss: 0.6389686465263367\n",
            "[Epoch 89 / 500], G Loss: 0.9089528322219849, D Loss: 0.603766918182373\n",
            "[Epoch 90 / 500], G Loss: 1.10237455368042, D Loss: 0.6184947490692139\n",
            "[Epoch 91 / 500], G Loss: 0.9040955305099487, D Loss: 0.5939621925354004\n",
            "[Epoch 92 / 500], G Loss: 0.8375375866889954, D Loss: 0.5994164347648621\n",
            "[Epoch 93 / 500], G Loss: 1.1570425033569336, D Loss: 0.5930950045585632\n",
            "[Epoch 94 / 500], G Loss: 0.7131012082099915, D Loss: 0.638135552406311\n",
            "[Epoch 95 / 500], G Loss: 0.8843547105789185, D Loss: 0.6029984354972839\n",
            "[Epoch 96 / 500], G Loss: 0.8496183156967163, D Loss: 0.5717737674713135\n",
            "[Epoch 97 / 500], G Loss: 1.2632931470870972, D Loss: 0.6112222671508789\n",
            "[Epoch 98 / 500], G Loss: 1.003645658493042, D Loss: 0.6335917711257935\n",
            "[Epoch 99 / 500], G Loss: 0.8668703436851501, D Loss: 0.5980347394943237\n",
            "[Epoch 100 / 500], G Loss: 0.7126603126525879, D Loss: 0.6109173893928528\n",
            "[Epoch 101 / 500], G Loss: 0.9335989356040955, D Loss: 0.5802392959594727\n",
            "[Epoch 102 / 500], G Loss: 0.9124873876571655, D Loss: 0.5868284702301025\n",
            "[Epoch 103 / 500], G Loss: 1.1644724607467651, D Loss: 0.6199784874916077\n",
            "[Epoch 104 / 500], G Loss: 0.9715254902839661, D Loss: 0.5794509649276733\n",
            "[Epoch 105 / 500], G Loss: 0.8827146291732788, D Loss: 0.6206052303314209\n",
            "[Epoch 106 / 500], G Loss: 1.0963413715362549, D Loss: 0.6075748801231384\n",
            "[Epoch 107 / 500], G Loss: 0.955641508102417, D Loss: 0.5818823575973511\n",
            "[Epoch 108 / 500], G Loss: 0.805717945098877, D Loss: 0.5887468457221985\n",
            "[Epoch 109 / 500], G Loss: 0.9867424964904785, D Loss: 0.5773736834526062\n",
            "[Epoch 110 / 500], G Loss: 1.0449955463409424, D Loss: 0.6198336482048035\n",
            "[Epoch 111 / 500], G Loss: 1.0002822875976562, D Loss: 0.5989589691162109\n",
            "[Epoch 112 / 500], G Loss: 1.1718473434448242, D Loss: 0.6068615913391113\n",
            "[Epoch 113 / 500], G Loss: 0.83852219581604, D Loss: 0.6021733283996582\n",
            "[Epoch 114 / 500], G Loss: 1.078281044960022, D Loss: 0.6012423634529114\n",
            "[Epoch 115 / 500], G Loss: 0.9017679691314697, D Loss: 0.5714271068572998\n",
            "[Epoch 116 / 500], G Loss: 1.2123268842697144, D Loss: 0.6453111171722412\n",
            "[Epoch 117 / 500], G Loss: 0.71131432056427, D Loss: 0.6304184198379517\n",
            "[Epoch 118 / 500], G Loss: 0.8725545406341553, D Loss: 0.5940002202987671\n",
            "[Epoch 119 / 500], G Loss: 0.7879563570022583, D Loss: 0.6001055836677551\n",
            "[Epoch 120 / 500], G Loss: 0.8395901918411255, D Loss: 0.6024784445762634\n",
            "[Epoch 121 / 500], G Loss: 0.6075225472450256, D Loss: 0.6585885882377625\n",
            "[Epoch 122 / 500], G Loss: 0.965623140335083, D Loss: 0.5911245346069336\n",
            "[Epoch 123 / 500], G Loss: 0.8094373345375061, D Loss: 0.5996627807617188\n",
            "[Epoch 124 / 500], G Loss: 1.3975310325622559, D Loss: 0.6370344161987305\n",
            "[Epoch 125 / 500], G Loss: 0.8330846428871155, D Loss: 0.5982117652893066\n",
            "[Epoch 126 / 500], G Loss: 0.9857406616210938, D Loss: 0.5719884634017944\n",
            "[Epoch 127 / 500], G Loss: 0.9076036810874939, D Loss: 0.6201952695846558\n",
            "[Epoch 128 / 500], G Loss: 0.8320102095603943, D Loss: 0.6023955941200256\n",
            "[Epoch 129 / 500], G Loss: 0.8789092302322388, D Loss: 0.6033871173858643\n",
            "[Epoch 130 / 500], G Loss: 0.9186846017837524, D Loss: 0.6122704744338989\n",
            "[Epoch 131 / 500], G Loss: 0.7721985578536987, D Loss: 0.6173427700996399\n",
            "[Epoch 132 / 500], G Loss: 0.8285973072052002, D Loss: 0.5751857161521912\n",
            "[Epoch 133 / 500], G Loss: 0.913148820400238, D Loss: 0.5947830080986023\n",
            "[Epoch 134 / 500], G Loss: 0.939285159111023, D Loss: 0.5879369974136353\n",
            "[Epoch 135 / 500], G Loss: 0.9712448120117188, D Loss: 0.615504264831543\n",
            "[Epoch 136 / 500], G Loss: 0.866950511932373, D Loss: 0.5980371236801147\n",
            "[Epoch 137 / 500], G Loss: 1.1201808452606201, D Loss: 0.5772019624710083\n",
            "[Epoch 138 / 500], G Loss: 0.6560274958610535, D Loss: 0.6249800324440002\n",
            "[Epoch 139 / 500], G Loss: 0.7032110691070557, D Loss: 0.6257646679878235\n",
            "[Epoch 140 / 500], G Loss: 1.250180959701538, D Loss: 0.6607038974761963\n",
            "[Epoch 141 / 500], G Loss: 0.8871672749519348, D Loss: 0.593321681022644\n",
            "[Epoch 142 / 500], G Loss: 0.8581748008728027, D Loss: 0.6025930643081665\n",
            "[Epoch 143 / 500], G Loss: 1.0568524599075317, D Loss: 0.5863686800003052\n",
            "[Epoch 144 / 500], G Loss: 0.7393548488616943, D Loss: 0.605762243270874\n",
            "[Epoch 145 / 500], G Loss: 0.8820109963417053, D Loss: 0.5974677801132202\n",
            "[Epoch 146 / 500], G Loss: 1.1207313537597656, D Loss: 0.592974841594696\n",
            "[Epoch 147 / 500], G Loss: 0.9826931953430176, D Loss: 0.5813105702400208\n",
            "[Epoch 148 / 500], G Loss: 0.9299708604812622, D Loss: 0.5738031268119812\n",
            "[Epoch 149 / 500], G Loss: 0.7747405767440796, D Loss: 0.6278740763664246\n",
            "[Epoch 150 / 500], G Loss: 1.0279488563537598, D Loss: 0.5938724279403687\n",
            "[Epoch 151 / 500], G Loss: 1.1557284593582153, D Loss: 0.6063623428344727\n",
            "[Epoch 152 / 500], G Loss: 1.3078858852386475, D Loss: 0.6229076385498047\n",
            "[Epoch 153 / 500], G Loss: 0.9161376953125, D Loss: 0.581248939037323\n",
            "[Epoch 154 / 500], G Loss: 0.5314092040061951, D Loss: 0.6705096364021301\n",
            "[Epoch 155 / 500], G Loss: 1.180152177810669, D Loss: 0.5905058979988098\n",
            "[Epoch 156 / 500], G Loss: 0.6846449375152588, D Loss: 0.6236268877983093\n",
            "[Epoch 157 / 500], G Loss: 0.7557699084281921, D Loss: 0.6023405194282532\n",
            "[Epoch 158 / 500], G Loss: 1.143648386001587, D Loss: 0.5740700364112854\n",
            "[Epoch 159 / 500], G Loss: 0.9430962204933167, D Loss: 0.5670225620269775\n",
            "[Epoch 160 / 500], G Loss: 1.0499670505523682, D Loss: 0.5775864124298096\n",
            "[Epoch 161 / 500], G Loss: 0.900769054889679, D Loss: 0.6002950668334961\n",
            "[Epoch 162 / 500], G Loss: 1.0922200679779053, D Loss: 0.5993639230728149\n",
            "[Epoch 163 / 500], G Loss: 1.1123937368392944, D Loss: 0.5682111978530884\n",
            "[Epoch 164 / 500], G Loss: 1.0579187870025635, D Loss: 0.5745306015014648\n",
            "[Epoch 165 / 500], G Loss: 1.1469035148620605, D Loss: 0.5772955417633057\n",
            "[Epoch 166 / 500], G Loss: 0.7754829525947571, D Loss: 0.6071849465370178\n",
            "[Epoch 167 / 500], G Loss: 0.9591754674911499, D Loss: 0.5835705995559692\n",
            "[Epoch 168 / 500], G Loss: 0.8491467833518982, D Loss: 0.5919861793518066\n",
            "[Epoch 169 / 500], G Loss: 0.992743968963623, D Loss: 0.600257158279419\n",
            "[Epoch 170 / 500], G Loss: 0.9350419640541077, D Loss: 0.6292694211006165\n",
            "[Epoch 171 / 500], G Loss: 0.9545547366142273, D Loss: 0.5571041107177734\n",
            "[Epoch 172 / 500], G Loss: 1.16654634475708, D Loss: 0.574354350566864\n",
            "[Epoch 173 / 500], G Loss: 0.9489678740501404, D Loss: 0.5853567719459534\n",
            "[Epoch 174 / 500], G Loss: 1.245172142982483, D Loss: 0.5956298112869263\n",
            "[Epoch 175 / 500], G Loss: 0.9993658065795898, D Loss: 0.5926180481910706\n",
            "[Epoch 176 / 500], G Loss: 0.9476073980331421, D Loss: 0.5770308971405029\n",
            "[Epoch 177 / 500], G Loss: 0.9162737131118774, D Loss: 0.6081836223602295\n",
            "[Epoch 178 / 500], G Loss: 0.8508356213569641, D Loss: 0.601009726524353\n",
            "[Epoch 179 / 500], G Loss: 0.9222986102104187, D Loss: 0.5764924883842468\n",
            "[Epoch 180 / 500], G Loss: 1.3404637575149536, D Loss: 0.6113121509552002\n",
            "[Epoch 181 / 500], G Loss: 0.6096354722976685, D Loss: 0.6434475779533386\n",
            "[Epoch 182 / 500], G Loss: 1.2560555934906006, D Loss: 0.5920076966285706\n",
            "[Epoch 183 / 500], G Loss: 1.1861246824264526, D Loss: 0.5687782764434814\n",
            "[Epoch 184 / 500], G Loss: 0.9510082006454468, D Loss: 0.5640439987182617\n",
            "[Epoch 185 / 500], G Loss: 1.2513259649276733, D Loss: 0.604195237159729\n",
            "[Epoch 186 / 500], G Loss: 1.0968165397644043, D Loss: 0.5757461786270142\n",
            "[Epoch 187 / 500], G Loss: 0.8582106828689575, D Loss: 0.621853232383728\n",
            "[Epoch 188 / 500], G Loss: 0.9806559681892395, D Loss: 0.592676043510437\n",
            "[Epoch 189 / 500], G Loss: 0.9907860159873962, D Loss: 0.547362208366394\n",
            "[Epoch 190 / 500], G Loss: 1.024843692779541, D Loss: 0.567772626876831\n",
            "[Epoch 191 / 500], G Loss: 1.0547734498977661, D Loss: 0.5667879581451416\n",
            "[Epoch 192 / 500], G Loss: 0.9917261600494385, D Loss: 0.5443128347396851\n",
            "[Epoch 193 / 500], G Loss: 1.2041715383529663, D Loss: 0.593897819519043\n",
            "[Epoch 194 / 500], G Loss: 0.9531939029693604, D Loss: 0.5616888999938965\n",
            "[Epoch 195 / 500], G Loss: 0.8387900590896606, D Loss: 0.6055353283882141\n",
            "[Epoch 196 / 500], G Loss: 0.9082695245742798, D Loss: 0.5772696733474731\n",
            "[Epoch 197 / 500], G Loss: 0.7097747325897217, D Loss: 0.6181563138961792\n",
            "[Epoch 198 / 500], G Loss: 1.1267714500427246, D Loss: 0.5646682977676392\n",
            "[Epoch 199 / 500], G Loss: 1.2867616415023804, D Loss: 0.5961852669715881\n",
            "[Epoch 200 / 500], G Loss: 0.9836115837097168, D Loss: 0.5742990970611572\n",
            "[Epoch 201 / 500], G Loss: 0.7932254672050476, D Loss: 0.583317220211029\n",
            "[Epoch 202 / 500], G Loss: 1.3816243410110474, D Loss: 0.5845059752464294\n",
            "[Epoch 203 / 500], G Loss: 0.8070842027664185, D Loss: 0.6042287945747375\n",
            "[Epoch 204 / 500], G Loss: 0.8064805865287781, D Loss: 0.5906321406364441\n",
            "[Epoch 205 / 500], G Loss: 0.8968645334243774, D Loss: 0.5688921809196472\n",
            "[Epoch 206 / 500], G Loss: 0.8235777616500854, D Loss: 0.6111184358596802\n",
            "[Epoch 207 / 500], G Loss: 1.0477343797683716, D Loss: 0.5843237638473511\n",
            "[Epoch 208 / 500], G Loss: 0.6987934112548828, D Loss: 0.6177644729614258\n",
            "[Epoch 209 / 500], G Loss: 1.0856975317001343, D Loss: 0.5383989810943604\n",
            "[Epoch 210 / 500], G Loss: 1.3580461740493774, D Loss: 0.6172970533370972\n",
            "[Epoch 211 / 500], G Loss: 1.0290544033050537, D Loss: 0.5991594195365906\n",
            "[Epoch 212 / 500], G Loss: 0.8218305706977844, D Loss: 0.5929604172706604\n",
            "[Epoch 213 / 500], G Loss: 0.9787279367446899, D Loss: 0.5611306428909302\n",
            "[Epoch 214 / 500], G Loss: 0.8753581047058105, D Loss: 0.5857788324356079\n",
            "[Epoch 215 / 500], G Loss: 1.268105149269104, D Loss: 0.5996965765953064\n",
            "[Epoch 216 / 500], G Loss: 1.2329283952713013, D Loss: 0.5644197463989258\n",
            "[Epoch 217 / 500], G Loss: 0.5244146585464478, D Loss: 0.7234045267105103\n",
            "[Epoch 218 / 500], G Loss: 1.0211963653564453, D Loss: 0.5535827875137329\n",
            "[Epoch 219 / 500], G Loss: 0.8629145622253418, D Loss: 0.5726920366287231\n",
            "[Epoch 220 / 500], G Loss: 0.9130415320396423, D Loss: 0.5842568874359131\n",
            "[Epoch 221 / 500], G Loss: 1.036812424659729, D Loss: 0.5683226585388184\n",
            "[Epoch 222 / 500], G Loss: 1.377581000328064, D Loss: 0.5817644596099854\n",
            "[Epoch 223 / 500], G Loss: 1.2089366912841797, D Loss: 0.5892736911773682\n",
            "[Epoch 224 / 500], G Loss: 1.0066193342208862, D Loss: 0.5329101085662842\n",
            "[Epoch 225 / 500], G Loss: 0.7371301054954529, D Loss: 0.6220887303352356\n",
            "[Epoch 226 / 500], G Loss: 0.8938732743263245, D Loss: 0.5971735715866089\n",
            "[Epoch 227 / 500], G Loss: 0.7670528888702393, D Loss: 0.6085322499275208\n",
            "[Epoch 228 / 500], G Loss: 1.1079909801483154, D Loss: 0.5955702662467957\n",
            "[Epoch 229 / 500], G Loss: 1.1508963108062744, D Loss: 0.5588351488113403\n",
            "[Epoch 230 / 500], G Loss: 0.84633469581604, D Loss: 0.5608160495758057\n",
            "[Epoch 231 / 500], G Loss: 0.9544118642807007, D Loss: 0.5594679117202759\n",
            "[Epoch 232 / 500], G Loss: 1.2716621160507202, D Loss: 0.5756673812866211\n",
            "[Epoch 233 / 500], G Loss: 1.2540946006774902, D Loss: 0.5513749122619629\n",
            "[Epoch 234 / 500], G Loss: 0.9290891289710999, D Loss: 0.5690944194793701\n",
            "[Epoch 235 / 500], G Loss: 0.8209928870201111, D Loss: 0.5983884334564209\n",
            "[Epoch 236 / 500], G Loss: 1.1574664115905762, D Loss: 0.574548602104187\n",
            "[Epoch 237 / 500], G Loss: 1.1550567150115967, D Loss: 0.558868408203125\n",
            "[Epoch 238 / 500], G Loss: 1.1564819812774658, D Loss: 0.5890179872512817\n",
            "[Epoch 239 / 500], G Loss: 1.1449068784713745, D Loss: 0.5780038833618164\n",
            "[Epoch 240 / 500], G Loss: 0.8623915314674377, D Loss: 0.5714471340179443\n",
            "[Epoch 241 / 500], G Loss: 0.8595907688140869, D Loss: 0.5806511640548706\n",
            "[Epoch 242 / 500], G Loss: 0.9579136371612549, D Loss: 0.5959358811378479\n",
            "[Epoch 243 / 500], G Loss: 1.1531779766082764, D Loss: 0.5658873319625854\n",
            "[Epoch 244 / 500], G Loss: 1.0341240167617798, D Loss: 0.5770750045776367\n",
            "[Epoch 245 / 500], G Loss: 1.2341245412826538, D Loss: 0.6424372792243958\n",
            "[Epoch 246 / 500], G Loss: 1.0810436010360718, D Loss: 0.5720361471176147\n",
            "[Epoch 247 / 500], G Loss: 1.4207106828689575, D Loss: 0.5823251008987427\n",
            "[Epoch 248 / 500], G Loss: 0.7407684922218323, D Loss: 0.6016668081283569\n",
            "[Epoch 249 / 500], G Loss: 1.1982111930847168, D Loss: 0.5478582382202148\n",
            "[Epoch 250 / 500], G Loss: 0.8885319232940674, D Loss: 0.6079318523406982\n",
            "[Epoch 251 / 500], G Loss: 0.576461672782898, D Loss: 0.6644920706748962\n",
            "[Epoch 252 / 500], G Loss: 1.0609543323516846, D Loss: 0.5954059958457947\n",
            "[Epoch 253 / 500], G Loss: 1.378678321838379, D Loss: 0.6042757034301758\n",
            "[Epoch 254 / 500], G Loss: 1.2194712162017822, D Loss: 0.5684337615966797\n",
            "[Epoch 255 / 500], G Loss: 1.1543145179748535, D Loss: 0.5645339488983154\n",
            "[Epoch 256 / 500], G Loss: 1.1271779537200928, D Loss: 0.5640028715133667\n",
            "[Epoch 257 / 500], G Loss: 0.9168930053710938, D Loss: 0.6113940477371216\n",
            "[Epoch 258 / 500], G Loss: 0.778874933719635, D Loss: 0.5994062423706055\n",
            "[Epoch 259 / 500], G Loss: 1.1356799602508545, D Loss: 0.5631262063980103\n",
            "[Epoch 260 / 500], G Loss: 1.132887363433838, D Loss: 0.5520769357681274\n",
            "[Epoch 261 / 500], G Loss: 0.842700183391571, D Loss: 0.5774329304695129\n",
            "[Epoch 262 / 500], G Loss: 0.9303972125053406, D Loss: 0.5721443891525269\n",
            "[Epoch 263 / 500], G Loss: 0.9260125756263733, D Loss: 0.5767759084701538\n",
            "[Epoch 264 / 500], G Loss: 1.2283265590667725, D Loss: 0.5861771106719971\n",
            "[Epoch 265 / 500], G Loss: 0.9183692932128906, D Loss: 0.5551007986068726\n",
            "[Epoch 266 / 500], G Loss: 0.7624837160110474, D Loss: 0.5787346363067627\n",
            "[Epoch 267 / 500], G Loss: 1.1791512966156006, D Loss: 0.5920418500900269\n",
            "[Epoch 268 / 500], G Loss: 0.9807633757591248, D Loss: 0.5709735751152039\n",
            "[Epoch 269 / 500], G Loss: 1.18825101852417, D Loss: 0.5594135522842407\n",
            "[Epoch 270 / 500], G Loss: 1.243538498878479, D Loss: 0.5589467287063599\n",
            "[Epoch 271 / 500], G Loss: 0.9500634074211121, D Loss: 0.5516987442970276\n",
            "[Epoch 272 / 500], G Loss: 1.0264701843261719, D Loss: 0.5615978240966797\n",
            "[Epoch 273 / 500], G Loss: 1.14058518409729, D Loss: 0.542567253112793\n",
            "[Epoch 274 / 500], G Loss: 1.2660605907440186, D Loss: 0.5637651681900024\n",
            "[Epoch 275 / 500], G Loss: 0.7688063383102417, D Loss: 0.6164436936378479\n",
            "[Epoch 276 / 500], G Loss: 1.114886999130249, D Loss: 0.5665509700775146\n",
            "[Epoch 277 / 500], G Loss: 0.9365378022193909, D Loss: 0.5627008080482483\n",
            "[Epoch 278 / 500], G Loss: 0.9754258394241333, D Loss: 0.5687394738197327\n",
            "[Epoch 279 / 500], G Loss: 0.9353156089782715, D Loss: 0.5575799942016602\n",
            "[Epoch 280 / 500], G Loss: 1.1849234104156494, D Loss: 0.5667650103569031\n",
            "[Epoch 281 / 500], G Loss: 1.1658260822296143, D Loss: 0.5606441497802734\n",
            "[Epoch 282 / 500], G Loss: 1.1530121564865112, D Loss: 0.5477858781814575\n",
            "[Epoch 283 / 500], G Loss: 0.9173797369003296, D Loss: 0.5614179968833923\n",
            "[Epoch 284 / 500], G Loss: 0.967077374458313, D Loss: 0.5388743877410889\n",
            "[Epoch 285 / 500], G Loss: 1.0558542013168335, D Loss: 0.5583975315093994\n",
            "[Epoch 286 / 500], G Loss: 1.1195851564407349, D Loss: 0.5700898170471191\n",
            "[Epoch 287 / 500], G Loss: 1.0409594774246216, D Loss: 0.5621130466461182\n",
            "[Epoch 288 / 500], G Loss: 1.5164480209350586, D Loss: 0.5854519009590149\n",
            "[Epoch 289 / 500], G Loss: 1.0498491525650024, D Loss: 0.5560139417648315\n",
            "[Epoch 290 / 500], G Loss: 0.7743955850601196, D Loss: 0.5988523364067078\n",
            "[Epoch 291 / 500], G Loss: 0.9935353994369507, D Loss: 0.5578250885009766\n",
            "[Epoch 292 / 500], G Loss: 0.9928346276283264, D Loss: 0.5698602199554443\n",
            "[Epoch 293 / 500], G Loss: 1.087924599647522, D Loss: 0.5503319501876831\n",
            "[Epoch 294 / 500], G Loss: 1.0676170587539673, D Loss: 0.5567759275436401\n",
            "[Epoch 295 / 500], G Loss: 1.2052428722381592, D Loss: 0.5436040163040161\n",
            "[Epoch 296 / 500], G Loss: 1.0268714427947998, D Loss: 0.5773142576217651\n",
            "[Epoch 297 / 500], G Loss: 1.2828949689865112, D Loss: 0.5489283800125122\n",
            "[Epoch 298 / 500], G Loss: 0.8925933837890625, D Loss: 0.581108033657074\n",
            "[Epoch 299 / 500], G Loss: 0.9943040609359741, D Loss: 0.5476555824279785\n",
            "[Epoch 300 / 500], G Loss: 1.2589467763900757, D Loss: 0.5823425650596619\n",
            "[Epoch 301 / 500], G Loss: 1.0807700157165527, D Loss: 0.5450298190116882\n",
            "[Epoch 302 / 500], G Loss: 0.9578656554222107, D Loss: 0.5555375218391418\n",
            "[Epoch 303 / 500], G Loss: 1.0492702722549438, D Loss: 0.5648056268692017\n",
            "[Epoch 304 / 500], G Loss: 0.7518705129623413, D Loss: 0.6013604402542114\n",
            "[Epoch 305 / 500], G Loss: 1.295689582824707, D Loss: 0.5803441405296326\n",
            "[Epoch 306 / 500], G Loss: 1.053231954574585, D Loss: 0.5630448460578918\n",
            "[Epoch 307 / 500], G Loss: 1.0076565742492676, D Loss: 0.5300183892250061\n",
            "[Epoch 308 / 500], G Loss: 0.8391582369804382, D Loss: 0.5693819522857666\n",
            "[Epoch 309 / 500], G Loss: 1.4959605932235718, D Loss: 0.5738069415092468\n",
            "[Epoch 310 / 500], G Loss: 1.0707242488861084, D Loss: 0.5734554529190063\n",
            "[Epoch 311 / 500], G Loss: 1.3520869016647339, D Loss: 0.595538318157196\n",
            "[Epoch 312 / 500], G Loss: 0.9677243232727051, D Loss: 0.5618610978126526\n",
            "[Epoch 313 / 500], G Loss: 1.1949021816253662, D Loss: 0.5466594696044922\n",
            "[Epoch 314 / 500], G Loss: 0.7326046228408813, D Loss: 0.5948363542556763\n",
            "[Epoch 315 / 500], G Loss: 1.1064672470092773, D Loss: 0.5755283832550049\n",
            "[Epoch 316 / 500], G Loss: 0.914888858795166, D Loss: 0.5553039312362671\n",
            "[Epoch 317 / 500], G Loss: 1.125565767288208, D Loss: 0.5337437987327576\n",
            "[Epoch 318 / 500], G Loss: 1.015023946762085, D Loss: 0.546167254447937\n",
            "[Epoch 319 / 500], G Loss: 0.8786416053771973, D Loss: 0.5754879713058472\n",
            "[Epoch 320 / 500], G Loss: 1.1181570291519165, D Loss: 0.5369510650634766\n",
            "[Epoch 321 / 500], G Loss: 0.9243568181991577, D Loss: 0.5718092322349548\n",
            "[Epoch 322 / 500], G Loss: 1.120679259300232, D Loss: 0.5566792488098145\n",
            "[Epoch 323 / 500], G Loss: 1.1777156591415405, D Loss: 0.5239754319190979\n",
            "[Epoch 324 / 500], G Loss: 1.3232344388961792, D Loss: 0.5413437485694885\n",
            "[Epoch 325 / 500], G Loss: 1.2736623287200928, D Loss: 0.5274299383163452\n",
            "[Epoch 326 / 500], G Loss: 0.7042726278305054, D Loss: 0.6505685448646545\n",
            "[Epoch 327 / 500], G Loss: 1.4178918600082397, D Loss: 0.5684589743614197\n",
            "[Epoch 328 / 500], G Loss: 1.2800815105438232, D Loss: 0.5384049415588379\n",
            "[Epoch 329 / 500], G Loss: 1.218923807144165, D Loss: 0.5766181945800781\n",
            "[Epoch 330 / 500], G Loss: 1.1202852725982666, D Loss: 0.5430386662483215\n",
            "[Epoch 331 / 500], G Loss: 1.0438530445098877, D Loss: 0.569501519203186\n",
            "[Epoch 332 / 500], G Loss: 1.0965182781219482, D Loss: 0.5488928556442261\n",
            "[Epoch 333 / 500], G Loss: 1.2084091901779175, D Loss: 0.552449643611908\n",
            "[Epoch 334 / 500], G Loss: 0.9546842575073242, D Loss: 0.5722843408584595\n",
            "[Epoch 335 / 500], G Loss: 0.8602935075759888, D Loss: 0.5687984824180603\n",
            "[Epoch 336 / 500], G Loss: 1.3752689361572266, D Loss: 0.5413638353347778\n",
            "[Epoch 337 / 500], G Loss: 1.1030384302139282, D Loss: 0.5333245992660522\n",
            "[Epoch 338 / 500], G Loss: 1.2715108394622803, D Loss: 0.5118746161460876\n",
            "[Epoch 339 / 500], G Loss: 1.110704779624939, D Loss: 0.571365237236023\n",
            "[Epoch 340 / 500], G Loss: 1.6382473707199097, D Loss: 0.5753228664398193\n",
            "[Epoch 341 / 500], G Loss: 1.303427815437317, D Loss: 0.5504704713821411\n",
            "[Epoch 342 / 500], G Loss: 0.9239329099655151, D Loss: 0.583725094795227\n",
            "[Epoch 343 / 500], G Loss: 1.2169222831726074, D Loss: 0.5128933191299438\n",
            "[Epoch 344 / 500], G Loss: 1.2362370491027832, D Loss: 0.5516057014465332\n",
            "[Epoch 345 / 500], G Loss: 0.9340494275093079, D Loss: 0.5819088816642761\n",
            "[Epoch 346 / 500], G Loss: 1.489787220954895, D Loss: 0.5582926273345947\n",
            "[Epoch 347 / 500], G Loss: 1.3864426612854004, D Loss: 0.534859299659729\n",
            "[Epoch 348 / 500], G Loss: 1.1374815702438354, D Loss: 0.5002489686012268\n",
            "[Epoch 349 / 500], G Loss: 1.3678680658340454, D Loss: 0.5260887742042542\n",
            "[Epoch 350 / 500], G Loss: 1.1951382160186768, D Loss: 0.5281320214271545\n",
            "[Epoch 351 / 500], G Loss: 1.0523369312286377, D Loss: 0.548151969909668\n",
            "[Epoch 352 / 500], G Loss: 0.9871094822883606, D Loss: 0.5923465490341187\n",
            "[Epoch 353 / 500], G Loss: 0.9234463572502136, D Loss: 0.5596712827682495\n",
            "[Epoch 354 / 500], G Loss: 0.9682994484901428, D Loss: 0.5462580323219299\n",
            "[Epoch 355 / 500], G Loss: 1.4211816787719727, D Loss: 0.548617422580719\n",
            "[Epoch 356 / 500], G Loss: 1.0363998413085938, D Loss: 0.5507182478904724\n",
            "[Epoch 357 / 500], G Loss: 1.395506501197815, D Loss: 0.5506119132041931\n",
            "[Epoch 358 / 500], G Loss: 0.9326085448265076, D Loss: 0.54811692237854\n",
            "[Epoch 359 / 500], G Loss: 1.1298216581344604, D Loss: 0.5136372447013855\n",
            "[Epoch 360 / 500], G Loss: 1.0414361953735352, D Loss: 0.5330873727798462\n",
            "[Epoch 361 / 500], G Loss: 0.8717834949493408, D Loss: 0.6056144833564758\n",
            "[Epoch 362 / 500], G Loss: 1.010733723640442, D Loss: 0.5594933032989502\n",
            "[Epoch 363 / 500], G Loss: 1.1320961713790894, D Loss: 0.5466236472129822\n",
            "[Epoch 364 / 500], G Loss: 1.077488660812378, D Loss: 0.5437630414962769\n",
            "[Epoch 365 / 500], G Loss: 1.2901077270507812, D Loss: 0.5500405430793762\n",
            "[Epoch 366 / 500], G Loss: 1.1365731954574585, D Loss: 0.5480355024337769\n",
            "[Epoch 367 / 500], G Loss: 0.7594606876373291, D Loss: 0.6129181981086731\n",
            "[Epoch 368 / 500], G Loss: 1.2197102308273315, D Loss: 0.5210449695587158\n",
            "[Epoch 369 / 500], G Loss: 1.1875412464141846, D Loss: 0.5333808660507202\n",
            "[Epoch 370 / 500], G Loss: 1.0561631917953491, D Loss: 0.5460601449012756\n",
            "[Epoch 371 / 500], G Loss: 1.011947512626648, D Loss: 0.5295268297195435\n",
            "[Epoch 372 / 500], G Loss: 1.0070574283599854, D Loss: 0.540144681930542\n",
            "[Epoch 373 / 500], G Loss: 1.3303803205490112, D Loss: 0.5647891163825989\n",
            "[Epoch 374 / 500], G Loss: 1.0134735107421875, D Loss: 0.5420354008674622\n",
            "[Epoch 375 / 500], G Loss: 1.3043627738952637, D Loss: 0.5233242511749268\n",
            "[Epoch 376 / 500], G Loss: 1.1648988723754883, D Loss: 0.4891928434371948\n",
            "[Epoch 377 / 500], G Loss: 0.8824139833450317, D Loss: 0.5888043642044067\n",
            "[Epoch 378 / 500], G Loss: 1.0862293243408203, D Loss: 0.5254210233688354\n",
            "[Epoch 379 / 500], G Loss: 1.103726863861084, D Loss: 0.5455926656723022\n",
            "[Epoch 380 / 500], G Loss: 1.183133840560913, D Loss: 0.5373179316520691\n",
            "[Epoch 381 / 500], G Loss: 1.0780528783798218, D Loss: 0.5210062861442566\n",
            "[Epoch 382 / 500], G Loss: 1.469419240951538, D Loss: 0.5452752113342285\n",
            "[Epoch 383 / 500], G Loss: 1.1538740396499634, D Loss: 0.5445535182952881\n",
            "[Epoch 384 / 500], G Loss: 0.9066450595855713, D Loss: 0.5647730827331543\n",
            "[Epoch 385 / 500], G Loss: 1.329184651374817, D Loss: 0.579125165939331\n",
            "[Epoch 386 / 500], G Loss: 1.1089215278625488, D Loss: 0.5458256006240845\n",
            "[Epoch 387 / 500], G Loss: 1.407039761543274, D Loss: 0.5203803181648254\n",
            "[Epoch 388 / 500], G Loss: 1.386897087097168, D Loss: 0.5271020531654358\n",
            "[Epoch 389 / 500], G Loss: 1.491860270500183, D Loss: 0.5071470141410828\n",
            "[Epoch 390 / 500], G Loss: 1.4194393157958984, D Loss: 0.5442306399345398\n",
            "[Epoch 391 / 500], G Loss: 1.0540456771850586, D Loss: 0.5150256752967834\n",
            "[Epoch 392 / 500], G Loss: 1.1806316375732422, D Loss: 0.5461941957473755\n",
            "[Epoch 393 / 500], G Loss: 1.2198917865753174, D Loss: 0.5235650539398193\n",
            "[Epoch 394 / 500], G Loss: 1.2395384311676025, D Loss: 0.5391749143600464\n",
            "[Epoch 395 / 500], G Loss: 1.3466265201568604, D Loss: 0.5275698304176331\n",
            "[Epoch 396 / 500], G Loss: 1.1388663053512573, D Loss: 0.5200139284133911\n",
            "[Epoch 397 / 500], G Loss: 0.9930912256240845, D Loss: 0.5136861801147461\n",
            "[Epoch 398 / 500], G Loss: 1.2719619274139404, D Loss: 0.540560245513916\n",
            "[Epoch 399 / 500], G Loss: 0.9163070917129517, D Loss: 0.5580003261566162\n",
            "[Epoch 400 / 500], G Loss: 0.9038318991661072, D Loss: 0.5739507079124451\n",
            "[Epoch 401 / 500], G Loss: 1.0835723876953125, D Loss: 0.5508583784103394\n",
            "[Epoch 402 / 500], G Loss: 1.2104109525680542, D Loss: 0.5219097137451172\n",
            "[Epoch 403 / 500], G Loss: 1.1682157516479492, D Loss: 0.5034313201904297\n",
            "[Epoch 404 / 500], G Loss: 1.0849801301956177, D Loss: 0.5040997862815857\n",
            "[Epoch 405 / 500], G Loss: 1.6502381563186646, D Loss: 0.5552576780319214\n",
            "[Epoch 406 / 500], G Loss: 1.450683355331421, D Loss: 0.49350082874298096\n",
            "[Epoch 407 / 500], G Loss: 1.1400213241577148, D Loss: 0.5295814275741577\n",
            "[Epoch 408 / 500], G Loss: 1.323584794998169, D Loss: 0.5304911732673645\n",
            "[Epoch 409 / 500], G Loss: 0.8956834077835083, D Loss: 0.5770050287246704\n",
            "[Epoch 410 / 500], G Loss: 1.038101315498352, D Loss: 0.521973729133606\n",
            "[Epoch 411 / 500], G Loss: 1.2123645544052124, D Loss: 0.5209740400314331\n",
            "[Epoch 412 / 500], G Loss: 1.4728785753250122, D Loss: 0.5295469760894775\n",
            "[Epoch 413 / 500], G Loss: 0.8581001162528992, D Loss: 0.5678833723068237\n",
            "[Epoch 414 / 500], G Loss: 1.056697964668274, D Loss: 0.5422058701515198\n",
            "[Epoch 415 / 500], G Loss: 1.1590675115585327, D Loss: 0.5228668451309204\n",
            "[Epoch 416 / 500], G Loss: 1.0725047588348389, D Loss: 0.5112142562866211\n",
            "[Epoch 417 / 500], G Loss: 1.2546956539154053, D Loss: 0.5482059717178345\n",
            "[Epoch 418 / 500], G Loss: 1.557423710823059, D Loss: 0.5635532140731812\n",
            "[Epoch 419 / 500], G Loss: 1.2539728879928589, D Loss: 0.4975461959838867\n",
            "[Epoch 420 / 500], G Loss: 1.1224886178970337, D Loss: 0.5284859538078308\n",
            "[Epoch 421 / 500], G Loss: 1.1593821048736572, D Loss: 0.53501957654953\n",
            "[Epoch 422 / 500], G Loss: 0.8420846462249756, D Loss: 0.5635225772857666\n",
            "[Epoch 423 / 500], G Loss: 1.2260581254959106, D Loss: 0.5174092650413513\n",
            "[Epoch 424 / 500], G Loss: 1.12663996219635, D Loss: 0.4905415177345276\n",
            "[Epoch 425 / 500], G Loss: 1.1101975440979004, D Loss: 0.520178496837616\n",
            "[Epoch 426 / 500], G Loss: 1.265419840812683, D Loss: 0.4951721429824829\n",
            "[Epoch 427 / 500], G Loss: 1.1891173124313354, D Loss: 0.48964449763298035\n",
            "[Epoch 428 / 500], G Loss: 1.1326804161071777, D Loss: 0.5106246471405029\n",
            "[Epoch 429 / 500], G Loss: 1.0537221431732178, D Loss: 0.5395336747169495\n",
            "[Epoch 430 / 500], G Loss: 1.0070987939834595, D Loss: 0.5481610894203186\n",
            "[Epoch 431 / 500], G Loss: 0.9459164142608643, D Loss: 0.552251935005188\n",
            "[Epoch 432 / 500], G Loss: 0.9710965752601624, D Loss: 0.5483243465423584\n",
            "[Epoch 433 / 500], G Loss: 1.218782663345337, D Loss: 0.5049928426742554\n",
            "[Epoch 434 / 500], G Loss: 1.622951865196228, D Loss: 0.5616757869720459\n",
            "[Epoch 435 / 500], G Loss: 1.1919581890106201, D Loss: 0.5347716808319092\n",
            "[Epoch 436 / 500], G Loss: 1.4085230827331543, D Loss: 0.4962046444416046\n",
            "[Epoch 437 / 500], G Loss: 1.0413322448730469, D Loss: 0.5170916318893433\n",
            "[Epoch 438 / 500], G Loss: 1.0460205078125, D Loss: 0.529308021068573\n",
            "[Epoch 439 / 500], G Loss: 1.1412347555160522, D Loss: 0.5347127318382263\n",
            "[Epoch 440 / 500], G Loss: 1.1971147060394287, D Loss: 0.5072940587997437\n",
            "[Epoch 441 / 500], G Loss: 1.33191978931427, D Loss: 0.516804575920105\n",
            "[Epoch 442 / 500], G Loss: 0.9667767286300659, D Loss: 0.5358941555023193\n",
            "[Epoch 443 / 500], G Loss: 1.4199479818344116, D Loss: 0.5014935731887817\n",
            "[Epoch 444 / 500], G Loss: 0.9488397836685181, D Loss: 0.5635332465171814\n",
            "[Epoch 445 / 500], G Loss: 1.4673489332199097, D Loss: 0.5268570780754089\n",
            "[Epoch 446 / 500], G Loss: 1.0720936059951782, D Loss: 0.5080286264419556\n",
            "[Epoch 447 / 500], G Loss: 1.2630395889282227, D Loss: 0.5027442574501038\n",
            "[Epoch 448 / 500], G Loss: 1.176613211631775, D Loss: 0.508797287940979\n",
            "[Epoch 449 / 500], G Loss: 1.3060410022735596, D Loss: 0.5323269963264465\n",
            "[Epoch 450 / 500], G Loss: 1.0924413204193115, D Loss: 0.5176677703857422\n",
            "[Epoch 451 / 500], G Loss: 0.8625266551971436, D Loss: 0.5832018256187439\n",
            "[Epoch 452 / 500], G Loss: 1.2429686784744263, D Loss: 0.5216879844665527\n",
            "[Epoch 453 / 500], G Loss: 0.8987149000167847, D Loss: 0.5405787825584412\n",
            "[Epoch 454 / 500], G Loss: 1.056395411491394, D Loss: 0.5097346305847168\n",
            "[Epoch 455 / 500], G Loss: 0.819973349571228, D Loss: 0.5794368386268616\n",
            "[Epoch 456 / 500], G Loss: 1.6750718355178833, D Loss: 0.5427695512771606\n",
            "[Epoch 457 / 500], G Loss: 1.1517583131790161, D Loss: 0.5246071815490723\n",
            "[Epoch 458 / 500], G Loss: 1.099541425704956, D Loss: 0.5157759189605713\n",
            "[Epoch 459 / 500], G Loss: 1.6168359518051147, D Loss: 0.5360150933265686\n",
            "[Epoch 460 / 500], G Loss: 1.0214911699295044, D Loss: 0.5268913507461548\n",
            "[Epoch 461 / 500], G Loss: 1.3032643795013428, D Loss: 0.4730190336704254\n",
            "[Epoch 462 / 500], G Loss: 1.39382004737854, D Loss: 0.49821406602859497\n",
            "[Epoch 463 / 500], G Loss: 1.2568153142929077, D Loss: 0.5177614688873291\n",
            "[Epoch 464 / 500], G Loss: 1.1126827001571655, D Loss: 0.5121424198150635\n",
            "[Epoch 465 / 500], G Loss: 1.381309151649475, D Loss: 0.5444075465202332\n",
            "[Epoch 466 / 500], G Loss: 0.9294645190238953, D Loss: 0.5183357000350952\n",
            "[Epoch 467 / 500], G Loss: 1.3148854970932007, D Loss: 0.505170464515686\n",
            "[Epoch 468 / 500], G Loss: 1.198917269706726, D Loss: 0.47323060035705566\n",
            "[Epoch 469 / 500], G Loss: 1.2947375774383545, D Loss: 0.5168944001197815\n",
            "[Epoch 470 / 500], G Loss: 1.254189133644104, D Loss: 0.5205913782119751\n",
            "[Epoch 471 / 500], G Loss: 1.518493890762329, D Loss: 0.5327214598655701\n",
            "[Epoch 472 / 500], G Loss: 1.613425374031067, D Loss: 0.5214662551879883\n",
            "[Epoch 473 / 500], G Loss: 1.4173698425292969, D Loss: 0.5162695646286011\n",
            "[Epoch 474 / 500], G Loss: 1.4582116603851318, D Loss: 0.5265442728996277\n",
            "[Epoch 475 / 500], G Loss: 0.9601995944976807, D Loss: 0.5261720418930054\n",
            "[Epoch 476 / 500], G Loss: 1.3806967735290527, D Loss: 0.5391950607299805\n",
            "[Epoch 477 / 500], G Loss: 1.6309571266174316, D Loss: 0.5499836802482605\n",
            "[Epoch 478 / 500], G Loss: 1.1789425611495972, D Loss: 0.4947604238986969\n",
            "[Epoch 479 / 500], G Loss: 1.157454013824463, D Loss: 0.5365375280380249\n",
            "[Epoch 480 / 500], G Loss: 0.9160043597221375, D Loss: 0.5532439947128296\n",
            "[Epoch 481 / 500], G Loss: 0.9458049535751343, D Loss: 0.5594786405563354\n",
            "[Epoch 482 / 500], G Loss: 1.0991928577423096, D Loss: 0.5312978625297546\n",
            "[Epoch 483 / 500], G Loss: 1.4742342233657837, D Loss: 0.5262703895568848\n",
            "[Epoch 484 / 500], G Loss: 1.2766313552856445, D Loss: 0.48807230591773987\n",
            "[Epoch 485 / 500], G Loss: 0.9367537498474121, D Loss: 0.5574163794517517\n",
            "[Epoch 486 / 500], G Loss: 1.175389289855957, D Loss: 0.5077141523361206\n",
            "[Epoch 487 / 500], G Loss: 1.0634939670562744, D Loss: 0.5173532366752625\n",
            "[Epoch 488 / 500], G Loss: 0.9154207110404968, D Loss: 0.5664352178573608\n",
            "[Epoch 489 / 500], G Loss: 0.9056892395019531, D Loss: 0.5803576707839966\n",
            "[Epoch 490 / 500], G Loss: 1.502414584159851, D Loss: 0.4994668662548065\n",
            "[Epoch 491 / 500], G Loss: 1.5033901929855347, D Loss: 0.5613031387329102\n",
            "[Epoch 492 / 500], G Loss: 1.3434330224990845, D Loss: 0.4842662215232849\n",
            "[Epoch 493 / 500], G Loss: 1.290314793586731, D Loss: 0.48931753635406494\n",
            "[Epoch 494 / 500], G Loss: 1.3502287864685059, D Loss: 0.46925443410873413\n",
            "[Epoch 495 / 500], G Loss: 0.6913567781448364, D Loss: 0.6534743309020996\n",
            "[Epoch 496 / 500], G Loss: 1.8591593503952026, D Loss: 0.556189775466919\n",
            "[Epoch 497 / 500], G Loss: 0.9907737970352173, D Loss: 0.5300959348678589\n",
            "[Epoch 498 / 500], G Loss: 1.455834150314331, D Loss: 0.5227915644645691\n",
            "[Epoch 499 / 500], G Loss: 1.5801496505737305, D Loss: 0.5384353399276733\n",
            "[Epoch 500 / 500], G Loss: 1.3308169841766357, D Loss: 0.48694953322410583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization Generated Images"
      ],
      "metadata": {
        "id": "jpTz2_sbZpog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = tf.random.normal([5, 100])\n",
        "\n",
        "generated_imgs = generator.predict(z)\n",
        "generated_imgs = generated_imgs.transpose((1, 0, 2, 3)).reshape(28, -1)\n",
        "\n",
        "plt.figure(figsize=(15, 4))\n",
        "plt.imshow(generated_imgs, 'gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "R77tR-k0V1UD",
        "outputId": "6e75ccee-a26b-485e-9c3d-bba7bff72d17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}